<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Spark学习总结," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark性能优化——开发调优">
<meta property="og:url" content="http://yoursite.com/2017/03/11/Spark/开发(代码)调优/index.html">
<meta property="og:site_name" content="Andone1cc">
<meta property="og:description" content="Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。">
<meta property="og:image" content="http://a1.qpic.cn/psb?/V12DoSDy3E1yI8/MsxqcqhywR.Ezeta4KSsQLOHzdBvMEXLRT8PG1bHTwc!/b/dCABAAAAAAAA&bo=GwMGAgAAAAADBz4!&rf=viewer_4">
<meta property="og:image" content="http://tech.meituan.com/img/spark-tuning/group-by-key-wordcount.png">
<meta property="og:image" content="http://tech.meituan.com/img/spark-tuning/reduce-by-key-wordcount.png">
<meta property="og:updated_time" content="2017-03-11T15:07:34.807Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark性能优化——开发调优">
<meta name="twitter:description" content="Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。">
<meta name="twitter:image" content="http://a1.qpic.cn/psb?/V12DoSDy3E1yI8/MsxqcqhywR.Ezeta4KSsQLOHzdBvMEXLRT8PG1bHTwc!/b/dCABAAAAAAAA&bo=GwMGAgAAAAADBz4!&rf=viewer_4">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/03/11/Spark/开发(代码)调优/"/>





  <title> Spark性能优化——开发调优 | Andone1cc </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Andone1cc</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一个有梦想的CS小白</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Startseite
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archiv
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/03/11/Spark/开发(代码)调优/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="QDU-scc">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andone1cc">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Spark性能优化——开发调优
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-03-11T21:49:43+08:00">
                2017-03-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Spark/" itemprop="url" rel="index">
                    <span itemprop="name">Spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/03/11/Spark/开发(代码)调优/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/11/Spark/开发(代码)调优/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Spark性能优化的第一步，就是要在开发Spark作业的过程中注意和应用一些性能优化的基本原则。开发调优，就是要让大家了解以下一些Spark基本开发原则，包括：RDD lineage设计、算子的合理使用、特殊操作的优化等。在开发过程中，时时刻刻都应该注意以上原则，并将这些原则根据具体的业务以及实际的应用场景，灵活地运用到自己的Spark作业中。</p>
<a id="more"></a>
<h3 id="原则一：避免创建重复的RDD"><a href="#原则一：避免创建重复的RDD" class="headerlink" title="原则一：避免创建重复的RDD"></a>原则一：避免创建重复的RDD</h3><p>通常来说，我们在开发一个spark作业时，首先是基于摸个数据源(比如Hive表或HDFS文件)创建一个初始的RDD;接着对这个RDD执行某个算子操作,然后得到下一个RDD;以此类推，循环往复，知道计算出最终我们想要的结果。在这个过程中，多个RDD会通过不同的算子操作(比如map、reduce等)串起来，这个”RDD串”就是<code>RDD lineage</code>，也就是”RDD的血缘关系链”。</p>
<p>我们在开发过程中要注意：对于同一份数据，只应该创建一个RDD,不能创建多个RDD来代表同一份数据。</p>
<p><b>一个简单的例子</b></p>
<pre><code><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。</span></div><div class="line"><span class="comment">//也就是说，需要对一份数据执行两次算子操作。</span></div><div class="line"><span class="comment">// 错误的做法：对于同一份数据执行多次算子操作时，创建多个RDD。</span></div><div class="line"><span class="comment">// 这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，</span></div><div class="line"><span class="comment">//然后分别对每个RDD都执行了一个算子操作。</span></div><div class="line"><span class="comment">// 这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；</span></div><div class="line"><span class="comment">//第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。</span></div><div class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</div><div class="line">rdd1.map(...)</div><div class="line"><span class="keyword">val</span> rdd2 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</div><div class="line">rdd2.reduce(...)</div><div class="line"><span class="comment">/* 正确的用法：对于一份数据执行多次算子操作时，只使用一个RDD。</span></div><div class="line"> 这种写法很明显比上一种写法要好多了，因为我们对于同一份数据只创建了一个RDD，</div><div class="line"> 然后对这一个RDD执行了多次算子操作。但是要注意到这里为止优化还没有结束，</div><div class="line"> 由于rdd1被执行了两次算子操作，第二次执行reduce操作的时候，还会再次从源头处</div><div class="line"> 重新计算一次rdd1的数据，因此还是会有重复计算的性能开销。</div><div class="line"> 要彻底解决这个问题，必须结合“原则三：对多次使用的RDD进行持久化”，</div><div class="line"> 才能保证一个RDD被多次使用时只被计算一次。</div><div class="line">**/</div><div class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</div><div class="line">rdd1.map(...)</div><div class="line">rdd1.reduce(...)</div></pre></td></tr></table></figure>
</code></pre><h3 id="原则二：尽可能复用同一个RDD"><a href="#原则二：尽可能复用同一个RDD" class="headerlink" title="原则二：尽可能复用同一个RDD"></a>原则二：尽可能复用同一个RDD</h3><p>除了要避免在开发过程中对一份完全相同的数据创建多个RDD之外，在对不同的数据执行算子操作时还要尽可能的复用一个RDD。比如说，有一个RDD的数据格式是key-value类型，另一个是单value类型的，这两个RDD的value数据是完全一样的。那么此时我们可以只使用key-value类型的那个RDD,因为其中已经包含了另一个的数据。对于类似这种多个RDD的数据有重叠或者包含的情况，我们应该尽量复用一个RDD,这样可以尽可能的减少RDD的数量，从而尽可能减少算子执行的次数。</p>
<p><b>一个简单的例子</b></p>
<pre><code><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**错误的做法。</span></div><div class="line">有一个&lt;Long, String&gt;格式的RDD，即rdd1。</div><div class="line">接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，</div><div class="line">而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。</div><div class="line">*/</div><div class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</div><div class="line"><span class="type">JavaRDD</span>&lt;<span class="type">String</span>&gt; rdd2 = rdd1.map(...)</div><div class="line"><span class="comment">// 分别对rdd1和rdd2执行了不同的算子操作。</span></div><div class="line">rdd1.reduceByKey(...)</div><div class="line">rdd2.map(...)</div><div class="line"><span class="comment">/**正确的做法。</span></div><div class="line">上面这个case中，其实rdd1和rdd2的区别无非就是数据格式不同而已，rdd2的数据</div><div class="line">完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。</div><div class="line">此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。</div><div class="line">其实在这种情况下完全可以复用同一个RDD。</div><div class="line">我们可以使用rdd1，既做reduceByKey操作，也做map操作。</div><div class="line"> 在进行第二个map操作时，只使用每个数据的tuple._2,也就是rdd1中的value值，即可。</div><div class="line">*/</div><div class="line"><span class="type">JavaPairRDD</span>&lt;<span class="type">Long</span>, <span class="type">String</span>&gt; rdd1 = ...</div><div class="line">rdd1.reduceByKey(...)</div><div class="line">rdd1.map(tuple._2...)</div><div class="line"><span class="comment">/**</span></div><div class="line">第二种方式相较于第一种方式而言，很明显减少了一次rdd2的计算开销。</div><div class="line">但是到这里为止，优化还没有结束，对rdd1我们还是执行了两次算子操作，</div><div class="line">rdd1实际上还是会被计算两次。因此还需要配合“原则三：</div><div class="line">对多次使用的RDD进行持久化”进行使用，才能保证一个RDD被多次使用时只被计算一次。</div><div class="line">*/</div></pre></td></tr></table></figure>
</code></pre><h3 id="原则三：对多次使用的RDD进行持久化"><a href="#原则三：对多次使用的RDD进行持久化" class="headerlink" title="原则三：对多次使用的RDD进行持久化"></a>原则三：对多次使用的RDD进行持久化</h3><p>Spark中对于一个RDD执行多次算子的默认原理是这样的：每次你对一个RDD执行一个算子操作时，都会重新从头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。</p>
<p>因此对于这种情况，我们的建议是：对多次使用的RDD进行<code>持久化</code>。此时Spark就会根据你的持久化策略，将RDD中的数据保存到内存或者磁盘中。以后每次对这个RDD进行算子操作时，都会直接从内存或磁盘中提取持久化的RDD数据，然后执行算子，而不会从源头处重新计算一遍这个RDD,再执行算子操作。</p>
<p><b>对多次使用的RDD进行持久化的代码示例</b></p>
<pre><code><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 如果要对一个RDD进行持久化，只要对这个RDD调用cache()和persist()即可。</span></div><div class="line"><span class="comment">//cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。</span></div><div class="line"><span class="comment">// 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，</span></div><div class="line"><span class="comment">//才会将这个rdd1从源头处计算一次。</span></div><div class="line"><span class="comment">//第二次执行reduce算子时,就会直接从内存中提取数据进行计算,不会重复计算一个rdd。</span></div><div class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>).cache()</div><div class="line">rdd1.map(...)</div><div class="line">rdd1.reduce(...)</div><div class="line"><span class="comment">// persist()方法表示：手动选择持久化级别，并使用指定的方式进行持久化。</span></div><div class="line"><span class="comment">//其中的_SER后缀表示，使用序列化的方式来保存RDD数据，此时RDD中的每个</span></div><div class="line"><span class="comment">//partition都会序列化成一个大的字节数组，然后再持久化到内存或磁盘中。</span></div><div class="line"><span class="comment">//序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存</span></div><div class="line"><span class="comment">//被持久化数据占用过多，从而发生频繁GC。</span></div><div class="line"><span class="keyword">val</span> rdd1 = sc.textFile(<span class="string">"hdfs://192.168.0.1:9000/hello.txt"</span>)</div><div class="line">.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</div><div class="line">rdd1.map(...)</div><div class="line">rdd1.reduce(...)</div></pre></td></tr></table></figure>
</code></pre><p>对于persist()方法而言，我们可以根据不同的业务场景选择不同的持久化级别。</p>
<p><img src="http://a1.qpic.cn/psb?/V12DoSDy3E1yI8/MsxqcqhywR.Ezeta4KSsQLOHzdBvMEXLRT8PG1bHTwc!/b/dCABAAAAAAAA&amp;bo=GwMGAgAAAAADBz4!&amp;rf=viewer_4" alt=""></p>
<p><b>如何选择一种最合适的持久化策略</b></p>
<blockquote>
<p>默认情况下，性能最高的当然是MEMORY_ONLY，但前提是你的内存必须足够足够大，可以绰绰有余地存放下整个RDD的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果RDD中数据比较多时（比如几十亿），直接用这种持久化级别，会导致JVM的OOM内存溢出异常。</p>
<p>如果使用MEMORY_ONLY级别时发生了内存溢出，那么建议尝试使用MEMORY_ONLY_SER级别。该级别会将RDD数据序列化后再保存在内存中，此时每个partition仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比MEMORY_ONLY多出来的性能开销，主要就是序列化与反序列化的开销。但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。此外，可能发生的问题同上，如果RDD中的数据量过多的话，还是可能会导致OOM内存溢出的异常。</p>
<p>如果纯内存的级别都无法使用，那么建议使用MEMORY_AND_DISK_SER策略，而不是MEMORY_AND_DISK策略。因为既然到了这一步，就说明RDD的数据量很大，内存无法完全放下。序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</p>
<p>通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，有时还不如重新计算一次所有RDD。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销，除非是要求作业的高可用性，否则不建议使用。</p>
</blockquote>
<h3 id="原则四：尽量避免使用shuffle类算子"><a href="#原则四：尽量避免使用shuffle类算子" class="headerlink" title="原则四：尽量避免使用shuffle类算子"></a>原则四：尽量避免使用shuffle类算子</h3><p>如果有可能的话，要尽量避免使用shuffle类算子。因为spark作业运行过程中，<code>最消耗性能的地方就是shuffle过程。</code>shuffle过程，简单来说，就是将分布在集群中多个节点上的同一个key,拉取到同一个节点上，进行聚合或者join等操作。比如reduceByKey,join等算子，都会触发shuffle操作。</p>
<p>shuffle过程中，各个节点上的相同key都会写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理key过多，导致内存不够存放，进而溢写到磁盘文件中。<code>因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。</code>磁盘IO和网络数据传输传输也是shuffle性能较差的主要原因。</p>
<p>因此在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。</p>
<p><b>Broadcast与map进行join代码示例</b></p>
<pre><code><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 传统的join操作会导致shuffle操作。因为两个RDD中，</span></div><div class="line"><span class="comment">//相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。</span></div><div class="line"><span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</div><div class="line"><span class="comment">// Broadcast+map的join操作，不会导致shuffle操作。</span></div><div class="line"><span class="comment">// 使用Broadcast将一个数据量较小的RDD作为广播变量。</span></div><div class="line"><span class="keyword">val</span> rdd2Data = rdd2.collect()</div><div class="line"><span class="keyword">val</span> rdd2DataBroadcast = sc.broadcast(rdd2Data)</div><div class="line"><span class="comment">// 在rdd1.map算子中，可以从rdd2DataBroadcast中，获取rdd2的所有数据。</span></div><div class="line"><span class="comment">// 然后进行遍历，如果发现rdd2中某条数据的key与rdd1的当前数据的key是相同的</span></div><div class="line"><span class="comment">//，那么就判定可以进行join。此时就可以根据自己需要的方式，</span></div><div class="line"><span class="comment">//将rdd1当前数据与rdd2中可以连接的数据，拼接在一起（String或Tuple）。</span></div><div class="line"><span class="keyword">val</span> rdd3 = rdd1.map(rdd2DataBroadcast.value()...)</div><div class="line"><span class="comment">//注意以上操作,建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。</span></div><div class="line"><span class="comment">// 因为每个Executor的内存中，都会驻留一份rdd2的全量数据。</span></div></pre></td></tr></table></figure>
</code></pre><h3 id="原则五：使用map-side预聚合的shuffle操作"><a href="#原则五：使用map-side预聚合的shuffle操作" class="headerlink" title="原则五：使用map-side预聚合的shuffle操作"></a>原则五：使用map-side预聚合的shuffle操作</h3><p>如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子。</p>
<blockquote>
<p>所谓的map-side预聚合，说的是在每个节点本地对相同的key进行一次聚合操作，类似于MapReduce中的本地combiner。</p>
</blockquote>
<p>map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。通常来说，在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。因为reduceByKey和aggregateByKey算子都会使用用户自定义的函数对每个节点本地的相同key进行预聚合。而groupByKey算子是不会进行预聚合的，全量的数据会在集群的各个节点之间分发和传输，性能相对来说比较差。</p>
<p>比如如下两幅图，就是典型的例子，分别基于<code>reduceByKey和groupByKey进行单词计数</code>。其中第一张图是groupByKey的原理图，可以看到，<code>没有进行任何本地聚合时，所有数据都会在集群节点之间传输</code>；第二张图是reduceByKey的原理图，可以看到，<code>每个节点本地的相同key数据，都进行了预聚合，然后才传输到其他节点上进行全局聚合</code>。</p>
<p><img src="http://tech.meituan.com/img/spark-tuning/group-by-key-wordcount.png" alt=""></p>
<p><img src="http://tech.meituan.com/img/spark-tuning/reduce-by-key-wordcount.png" alt=""></p>
<h3 id="原则六：使用高性能的算子"><a href="#原则六：使用高性能的算子" class="headerlink" title="原则六：使用高性能的算子"></a>原则六：使用高性能的算子</h3><p>除了shuffle相关的算子有优化原则之外，其他的算子也都有着相应的优化原则。</p>
<p><b>1.使用reduceByKey/aggregateByKey替代groupByKey</b></p>
<p><b>2.使用mapPartitions替代普通map</b></p>
<p>mapPartitions类的算子，<code>一次函数调用会处理一个partition所有的数据，而不是一次函数调用处理一条，性能相对来说会高一些</code>。但是有的时候，使用mapPartitions会出现OOM（内存溢出）的问题。因为单次函数调用就要处理掉一个partition所有的数据，如果内存不够，垃圾回收时是无法回收掉太多对象的，很可能出现OOM异常。所以使用这类操作时要慎重！</p>
<p><b>3.使用foreachPartitions替代foreach</b></p>
<p>原理类似于“使用mapPartitions替代map”，也是一次函数调用处理一个partition的所有数据，而不是一次函数调用处理一条数据。</p>
<p>在实践中发现，foreachPartitions类的算子，对性能的提升还是很有帮助的。<code>比如在foreach函数中，将RDD中所有数据写MySQL，那么如果是普通的foreach算子，就会一条数据一条数据地写，每次函数调用可能就会创建一个数据库连接，此时就势必会频繁地创建和销毁数据库连接，性能是非常低下</code>；但是<code>如果用foreachPartitions算子一次性处理一个partition的数据，那么对于每个partition，只要创建一个数据库连接即可，然后执行批量插入操作，此时性能是比较高的</code>。实践中发现，对于1万条左右的数据量写MySQL，性能可以提升30%以上。</p>
<p><b>4.使用filter之后进行coalesce操作</b></p>
<p><code>通常对一个RDD执行filter算子过滤掉RDD中较多数据后（比如30%以上的数据），建议使用coalesce算子，手动减少RDD的partition数量</code>，将RDD中的数据压缩到更少的partition中去。因为filter之后，<code>RDD的每个partition中都会有很多数据被过滤掉</code>，此时如果照常进行后续的计算，其实每个task处理的partition中的数据量并不是很多，有一点资源浪费，而且此时处理的task越多，可能速度反而越慢。因此用coalesce减少partition数量，将RDD中的数据压缩到更少的partition之后，只要使用更少的task即可处理完所有的partition。在某些场景下，对于性能的提升会有一定的帮助。</p>
<p><b>5.使用repartitionAndSortWithinPartitions替代repartition与sort类操作</b></p>
<p>repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子。<code>因为该算子可以一边进行重分区的shuffle操作，一边进行排序。shuffle与sort两个操作同时进行，比先shuffle再sort来说，性能可能是要高的</code>。</p>
<p>【代码示例】</p>
<pre><code><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">object</span> <span class="title">RepartitionAndSortWithinPartitions</span> </span>&#123;</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</div><div class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</div><div class="line">    conf.setMaster(<span class="string">"local"</span>)</div><div class="line">    conf.setAppName(<span class="string">"RepartitionAndSortWithinPartitions"</span>)</div><div class="line">    conf.set(<span class="string">"spark.testing.memory"</span>, <span class="string">"2147480000"</span>)</div><div class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</div><div class="line">    <span class="keyword">val</span> data = <span class="type">Array</span>(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">5</span>)</div><div class="line">    <span class="keyword">val</span> rdd1 = sc.parallelize(data)</div><div class="line">    <span class="keyword">val</span> random = <span class="keyword">new</span> <span class="type">Random</span>()</div><div class="line">    <span class="keyword">val</span> rdd2 = rdd1.map(x =&gt; (x, random.nextInt(<span class="number">10</span>)))</div><div class="line">    <span class="comment">//rdd2.foreach(println)</span></div><div class="line">    rdd2.repartitionAndSortWithinPartitions(<span class="keyword">new</span> <span class="type">MyParttitioner</span>(<span class="number">3</span>))</div><div class="line">      .foreach(println)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyParttitioner</span>(<span class="params">numPartition:<span class="type">Int</span></span>)  <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span> </span>= numPartition</div><div class="line"></div><div class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>) = &#123;</div><div class="line">    <span class="type">Integer</span>.valueOf(key+<span class="string">""</span>) % numPartitions</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</code></pre><h3 id="原则七：广播大变量"><a href="#原则七：广播大变量" class="headerlink" title="原则七：广播大变量"></a>原则七：广播大变量</h3><p>有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。</p>
<p><code>在算子函数中使用到外部变量时，默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。</code>如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。</p>
<p>因此对于上述情况，如果使用的外部变量比较大，建议使用Spark的广播功能，对该变量进行广播。广播后的变量，<code>会保证每个Executor的内存中，只驻留一份变量副本</code>，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</p>
<p><b>广播大变量的代码示例</b></p>
<pre><code><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 以下代码在算子函数中，使用了外部的变量。</span></div><div class="line"><span class="comment">// 此时没有做任何特殊操作，每个task都会有一份list1的副本。</span></div><div class="line"><span class="keyword">val</span> list1 = ...</div><div class="line">rdd1.map(list1...)</div><div class="line"></div><div class="line"><span class="comment">// 以下代码将list1封装成了Broadcast类型的广播变量。</span></div><div class="line"><span class="comment">// 在算子函数中，使用广播变量时，首先会判断当前task所在Executor内存中，是否有变量副本。</span></div><div class="line"><span class="comment">// 如果有则直接使用；如果没有则从Driver或者其他Executor节点</span></div><div class="line"><span class="comment">//上远程拉取一份放到本地Executor内存中。</span></div><div class="line"><span class="comment">// 每个Executor内存中，就只会驻留一份广播变量副本。</span></div><div class="line"><span class="keyword">val</span> list1 = ...</div><div class="line"><span class="keyword">val</span> list1Broadcast = sc.broadcast(list1)</div><div class="line">rdd1.map(list1Broadcast...)</div></pre></td></tr></table></figure>
</code></pre><h3 id="原则八：使用Kryo优化序列化性能"><a href="#原则八：使用Kryo优化序列化性能" class="headerlink" title="原则八：使用Kryo优化序列化性能"></a>原则八：使用Kryo优化序列化性能</h3><p>在Spark中，主要有三个地方涉及到了序列化：</p>
<blockquote>
<p>在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输（见“原则七：广播大变量”中的讲解）。</p>
<p>将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。</p>
<p>使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。</p>
</blockquote>
<p>对于这三种出现序列化的地方，我们都可以通过使用<code>Kryo序列化类库</code>，来优化序列化和反序列化的性能。</p>
<p><code>Spark默认使用的是Java的序列化机制，也就是ObjectOutputStream/ObjectInputStream API来进行序列化和反序列化。</code><br>但是Spark同时支持使用Kryo序列化库，Kryo序列化类库的性能比Java序列化类库的性能要高很多。官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右。Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好要注册所有需要进行序列化的自定义类型，因此对于开发者来说，这种方式比较麻烦。</p>
<p>以下是使用Kryo的代码示例，我们只要设置序列化类，再注册要序列化的自定义类型即可（比如算子函数中使用到的外部变量类型、作为RDD泛型类型的自定义类型等）：</p>
<pre><code><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 创建SparkConf对象。</span></div><div class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(...).setAppName(...)</div><div class="line"><span class="comment">// 设置序列化器为KryoSerializer。</span></div><div class="line">conf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</div><div class="line"><span class="comment">// 注册要序列化的自定义类型。</span></div><div class="line">conf.registerKryoClasses(<span class="type">Array</span>(classOf[<span class="type">MyClass1</span>], classOf[<span class="type">MyClass2</span>]))</div></pre></td></tr></table></figure>
</code></pre><h3 id="原则九：优化数据结构"><a href="#原则九：优化数据结构" class="headerlink" title="原则九：优化数据结构"></a>原则九：优化数据结构</h3><p>Java中，有三种类型比较耗费内存：</p>
<blockquote>
<p>对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。</p>
<p>字符串，每个字符串内部都有一个字符数组以及长度等额外信息。</p>
<p>集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.Entry。</p>
</blockquote>
<p>因此Spark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用<code>字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，</code>这样尽可能地减少内存占用，从而降低GC频率，提升性能。</p>
<p>但是在笔者的编码实践中发现，要做到该原则其实并不容易。因为我们同时要考虑到代码的可维护性，如果一个代码中，完全没有任何对象抽象，全部是字符串拼接的方式，那么对于后续的代码维护和修改，无疑是一场巨大的灾难。同理，如果所有操作都基于数组实现，而不使用HashMap、LinkedList等集合类型，那么对于我们的编码难度以及代码可维护性，也是一个极大的挑战。因此笔者建议，<code>在可能以及合适的情况下，使用占用内存较少的数据结构，但是前提是要保证代码的可维护性</code>。</p>
<h3 id="原则十：使用高性能的库fastutil"><a href="#原则十：使用高性能的库fastutil" class="headerlink" title="原则十：使用高性能的库fastutil"></a>原则十：使用高性能的库fastutil</h3><p>fastutil介绍：</p>
<blockquote>
<p>fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue；</p>
<p>fastutil能够提供更小的内存占用，更快的存取速度；我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，好处在于，fastutil集合类，可以减小内存的占用，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度；</p>
<p>fastutil最新版本要求Java 7以及以上版本；</p>
<p>fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。</p>
</blockquote>
<p>【参考资料】</p>
<p><a href="http://tech.meituan.com/spark-tuning-basic.html" target="_blank" rel="external">http://tech.meituan.com/spark-tuning-basic.html</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Spark学习总结/" rel="tag"># Spark学习总结</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/03/07/Spark/sparkstreaming算子 /" rel="next" title="Spark学习笔记——Spark Streaming中transform,updateStateByKey,Window算子解析">
                <i class="fa fa-chevron-left"></i> Spark学习笔记——Spark Streaming中transform,updateStateByKey,Window算子解析
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/03/12/Spark/资源调优/" rel="prev" title="Spark性能优化——资源调优">
                Spark性能优化——资源调优 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2017/03/11/Spark/开发(代码)调优/"
           data-title="Spark性能优化——开发调优" data-url="http://yoursite.com/2017/03/11/Spark/开发(代码)调优/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Inhaltsverzeichnis
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Übersicht
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="QDU-scc" />
          <p class="site-author-name" itemprop="name">QDU-scc</p>
           
              <p class="site-description motion-element" itemprop="description">一个有梦想的CS小白</p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">52</span>
                <span class="site-state-item-name">Artikel</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">Kategorien</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">Tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#原则一：避免创建重复的RDD"><span class="nav-number">1.</span> <span class="nav-text">原则一：避免创建重复的RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原则二：尽可能复用同一个RDD"><span class="nav-number">2.</span> <span class="nav-text">原则二：尽可能复用同一个RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原则三：对多次使用的RDD进行持久化"><span class="nav-number">3.</span> <span class="nav-text">原则三：对多次使用的RDD进行持久化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原则四：尽量避免使用shuffle类算子"><span class="nav-number">4.</span> <span class="nav-text">原则四：尽量避免使用shuffle类算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原则五：使用map-side预聚合的shuffle操作"><span class="nav-number">5.</span> <span class="nav-text">原则五：使用map-side预聚合的shuffle操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原则六：使用高性能的算子"><span class="nav-number">6.</span> <span class="nav-text">原则六：使用高性能的算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原则七：广播大变量"><span class="nav-number">7.</span> <span class="nav-text">原则七：广播大变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原则八：使用Kryo优化序列化性能"><span class="nav-number">8.</span> <span class="nav-text">原则八：使用Kryo优化序列化性能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原则九：优化数据结构"><span class="nav-number">9.</span> <span class="nav-text">原则九：优化数据结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#原则十：使用高性能的库fastutil"><span class="nav-number">10.</span> <span class="nav-text">原则十：使用高性能的库fastutil</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">QDU-scc</span>
</div>


<div class="powered-by">
  Erstellt mit  <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"andone1cc"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  













  
  

  

  

  

  


  

</body>
</html>
